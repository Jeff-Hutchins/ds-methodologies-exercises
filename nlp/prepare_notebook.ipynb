{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Exercises\n",
    "\n",
    "The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n",
    "\n",
    "1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    " - Lowercase everything\n",
    " - Normalize unicode characters\n",
    " - Replace anything that is not a letter, number, whitespace, or a single quote.\n",
    " \n",
    "\n",
    "2. Define a function named tokenize. It should take in a string and tokenize all the words in the string.\n",
    "\n",
    "3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words.\n",
    "\n",
    "4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word.\n",
    "\n",
    "5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove.\n",
    "\n",
    "6. Define a function named prep_article that takes in the dictionary representing an article and returns a dictionary that looks like this:\n",
    "\n",
    "{\n",
    "\n",
    "    'title': 'the original title'.\n",
    "    \n",
    "    'original': original,\n",
    "    \n",
    "    'stemmed': article_stemmed,\n",
    "    \n",
    "    'lemmatized': article_lemmatized,\n",
    "    \n",
    "    'clean': article_without_stopwords\n",
    "}\n",
    "\n",
    "Note that if the orignal dictionary has a title property, it should remain unchanged (same goes for the category property).\n",
    "\n",
    "7.  Define a function named prepare_article_data that takes in the list of articles dictionaries, applies the prep_article function to each one, and returns the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import acquire\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles, df = acquire.get_blog_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean using dataframe of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is He...</td>\n",
       "      <td>\\nThe rumors are true! The time has arrived. C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths - Codeup</td>\n",
       "      <td>\\nBy Dimitri Antoniou and Maggie Giust\\nData S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>\\nBy Dimitri Antoniou\\nA week ago, Codeup laun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair - ...</td>\n",
       "      <td>\\n10 Tips to Crush It at the SA Tech Job Fair\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>\\nCompetitor Bootcamps Are Closing. Is the Mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Codeup’s Data Science Career Accelerator is He...   \n",
       "1                        Data Science Myths - Codeup   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...   \n",
       "3  10 Tips to Crush It at the SA Tech Job Fair - ...   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...   \n",
       "\n",
       "                                                body  \n",
       "0  \\nThe rumors are true! The time has arrived. C...  \n",
       "1  \\nBy Dimitri Antoniou and Maggie Giust\\nData S...  \n",
       "2  \\nBy Dimitri Antoniou\\nA week ago, Codeup laun...  \n",
       "3  \\n10 Tips to Crush It at the SA Tech Job Fair\\...  \n",
       "4  \\nCompetitor Bootcamps Are Closing. Is the Mod...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bodies = df.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \\nThe rumors are true! The time has arrived. C...\n",
       "1    \\nBy Dimitri Antoniou and Maggie Giust\\nData S...\n",
       "2    \\nBy Dimitri Antoniou\\nA week ago, Codeup laun...\n",
       "3    \\n10 Tips to Crush It at the SA Tech Job Fair\\...\n",
       "4    \\nCompetitor Bootcamps Are Closing. Is the Mod...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic_clean takes in a string and apply some basic text cleaning to it\n",
    "def basic_clean(string):\n",
    "    '''\n",
    "    Put doctring comments here. That way I can reference it for later to explain my function\n",
    "    '''\n",
    "    \n",
    "    # lowercase capitalized letters\n",
    "    string = string.lower()\n",
    "    \n",
    "    # normalizing the data by removing non-ASCII characters\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "        .encode('ascii', 'ignore')\\\n",
    "        .decode('utf-8', 'ignore')\n",
    "\n",
    "    # remove whitespace\n",
    "    string = string.strip()\n",
    "\n",
    "    # remove anything that is not a through z, a number, a single quote, or whitespace\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", '', string)\n",
    "    \n",
    "    # convert newlins and tabs to a single space\n",
    "    string = re.sub(r'[\\r|\\n|\\r\\n]+',' ', string)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    the rumors are true the time has arrived codeu...\n",
       "1    by dimitri antoniou and maggie giust data scie...\n",
       "2    by dimitri antoniou a week ago codeup launched...\n",
       "3    10 tips to crush it at the sa tech job fair sa...\n",
       "4    competitor bootcamps are closing is the model ...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.body.apply(basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize takes in a string and tokenize all the words in the string\n",
    "def tokenize():\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    return tokenizer.tokenize(string, return_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem accepts some text and return the text after applying stemming to all the words.\n",
    "def stem():\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    string_of_stems = ' '.join(stems)\n",
    "    return string_of_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize accepts some text and return the text after applying lemmatization to each word\n",
    "def lemmatize():\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    string_of_lemmas = ' '.join(lemmas)\n",
    "    return string_of_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_stopwords accepts some text and return the text after removing all the stopwords.\n",
    "def remove_stopwords():\n",
    "    # Tokenize the string\n",
    "    string = tokenize(string)\n",
    "\n",
    "    words = string.split()\n",
    "    stopword_list = stopwords.words('english')\n",
    "\n",
    "    # remove the excluded words from the stopword list\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "\n",
    "    # add in the user specified extra words\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    final_string = \" \".join(filtered_words)\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_article that takes in the dictionary representing an article and returns a dictionary:  Reference top cell for example of what it should look like.\n",
    "def prep_article():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_article_data that takes in the list of articles dictionaries, applies the prep_article function to each one, and returns the transformed data.\n",
    "def prepare_article_data():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean using string data ( needs a for loop to go through all of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1 = articles[0]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all capitalized letters\n",
    "article1 = article1.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the data by removing non-ASCII characters\n",
    "article1 = unicodedata.normalize('NFKD', article1)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "print(article1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespace\n",
    "article1 = article1.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove anything that is not a through z, a number, a single quote, or whitespace\n",
    "article1 = re.sub(r\"[^a-z0-9'\\s]\", '', article1)\n",
    "print(article1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic_clean takes in a string and apply some basic text cleaning to it\n",
    "def basic_clean(string):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize takes in a string and tokenize all the words in the string\n",
    "def tokenize():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem accepts some text and return the text after applying stemming to all the words.\n",
    "def stem():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize accepts some text and return the text after applying lemmatization to each word\n",
    "def lemmatize():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_stopwords accepts some text and return the text after removing all the stopwords.\n",
    "def remove_stopwords():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_article that takes in the dictionary representing an article and returns a dictionary:  Reference top cell for example of what it should look like.\n",
    "def prep_article():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_article_data that takes in the list of articles dictionaries, applies the prep_article function to each one, and returns the transformed data.\n",
    "def prepare_article_data():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
